{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "from scipy.sparse import coo_matrix,csr_matrix,csc_matrix,save_npz,load_npz\n",
    "import string\n",
    "from collections import Counter\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook contains all the commands to push data into database\n",
    "#### Indeed, all the data produced in the other notebooks is saved into json files\n",
    "#### The aim of this notebook is to process each one of these json files and add the data into database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains all the source and target of each link in the articles\n",
    "f = open('../Database/link_dict_inverse.json')\n",
    "link_dict_inverse = json.load(f)\n",
    "f.close()\n",
    "link_dict_inverse={int(key):list(map(int, value)) for key,value in link_dict_inverse.items()}\n",
    "\n",
    "conn = sqlite3.connect('../Database/Database.db')\n",
    "cursor = conn.cursor()\n",
    "# cursor.execute(\"Drop Table Articles\")\n",
    "create_db=\"CREATE TABLE Link_Dict_Inverse(ID_target NUMERIC,ID_source NUMERIC);\"\n",
    "cursor.execute(create_db)\n",
    "index=\"CREATE INDEX index_id_target ON Link_Dict_Inverse (ID_target);\"\n",
    "cursor.execute(index)\n",
    "\n",
    "for key,value in link_dict_inverse.items():\n",
    "    for source in value:\n",
    "        request='Insert into Link_Dict_Inverse Values (%d,%d)'%(key,source)\n",
    "        cursor.execute(request)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Hyacinthe Cartuyvels (1849-1897)', 'https://fr.wikipedia.org/wiki?curid=6797291')\n"
     ]
    }
   ],
   "source": [
    "# this file contains all the informations of each article\n",
    "# ID ROW = The row in the tf-idf, M, C matrices ; ID article : the real id of the article ; \n",
    "# title = title of the article ; url :url; location : absolute path towards the file containing the wikipedia article;\n",
    "# line_index : the line into the file (each line of the file is a wikipedia article) \n",
    "f = open('../Database/dataset.json')\n",
    "dataset = json.load(f)\n",
    "f.close()\n",
    "dataset={int(key):[value[0],int(value[1]),value[2],value[3],value[4]] for key,value in dataset.items()}\n",
    "\n",
    "conn = sqlite3.connect('../Database/Database.db')\n",
    "cursor = conn.cursor()\n",
    "# cursor.execute(\"Drop Table Articles\")\n",
    "create_db=\"CREATE TABLE Articles(ID_ROW NUMERIC,ID_ARTICLE NUMERIC,Title TEXT,URL TEXT,Location TEXT,Line_index NUMERIC);\"\n",
    "cursor.execute(create_db)\n",
    "index=\"CREATE INDEX index_id_article2 ON Articles (ID_ARTICLE);\"\n",
    "cursor.execute(index)\n",
    "index=\"CREATE INDEX index_id ON Articles (ID_ROW);\"\n",
    "cursor.execute(index)\n",
    "\n",
    "\n",
    "sq=\"'\" #single quote\n",
    "for key,value in dataset.items():\n",
    "    tmp=value[1].replace(\"'\",\" \")\n",
    "    request='Insert into Articles Values (%s,%s,%s,%s,%s,%s)'%(key,value[0],sq+tmp+sq,sq+value[2]+sq,sq+value[3]+sq,value[4])\n",
    "    cursor.execute(request)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the page rank vector of the articles\n",
    "r=np.load('../Database/r.npy')\n",
    "\n",
    "conn=sqlite3.connect('../Database/Database.db')\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"DROP TABLE R\")\n",
    "create_db=\"CREATE TABLE R(ID NUMERIC,Value TEXT);\"\n",
    "cursor.execute(create_db)\n",
    "index=\"CREATE INDEX index_id_r ON R(ID);\"\n",
    "cursor.execute(index)\n",
    "\n",
    "sq=\"'\"\n",
    "for i in range(len(r)):\n",
    "    request='Insert into R Values (%d,%s)'%(i,sq+str(r[i,0])+sq)\n",
    "    cursor.execute(request)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this file contains for each article its id and the index of the row associated in all matrices (R for Page Rank,\n",
    "# C for tf-idf, M for tf-idf, word2vec vectors)\n",
    "f = open('../Database/indices_dict.json')\n",
    "indices_dict = json.load(f)\n",
    "f.close()\n",
    "\n",
    "conn=sqlite3.connect('../Database/Database.db')\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"Drop table Transitional_id_article_index_R\")\n",
    "create_db=\"CREATE TABLE Transitional_id_article_index_R(ID_ARTICLE NUMERIC,INDEX_ROW NUMERIC);\"\n",
    "cursor.execute(create_db)\n",
    "index=\"CREATE INDEX index_id_article ON Transitional_id_article_index_R(ID_ARTICLE);\"\n",
    "cursor.execute(index)\n",
    "\n",
    "for key,value in indices_dict.items():\n",
    "    request='Insert into Transitional_id_article_index_R Values (%d,%d)'%(int(key),value)\n",
    "    cursor.execute(request)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100000\n",
      "200000\n",
      "300000\n"
     ]
    }
   ],
   "source": [
    "# new_voc_dict contains all the words of the tf-idf matrix (each word is a column) as key \n",
    "# and the index of the column as value\n",
    "# M is the M matrix in the tf-idf process. Contains for each cell the number of times a word is seen in an article\n",
    "\n",
    "f = open('../Database/new_voc_dict.json')\n",
    "new_voc_dict = json.load(f)\n",
    "f.close()\n",
    "M = load_npz(\"../Database/M.npz\")\n",
    "somme=np.sum(M,axis=0)\n",
    "\n",
    "conn=sqlite3.connect('../Database/Database.db')\n",
    "cursor=conn.cursor()\n",
    "cursor.execute(\"Drop table Vocabulary\")\n",
    "create_db=\"CREATE TABLE Vocabulary(WORD TEXT,INDEX_COLUMN NUMERIC, M_SUM NUMERIC);\"\n",
    "cursor.execute(create_db)\n",
    "index=\"CREATE INDEX index_id_word ON Vocabulary(WORD);\"\n",
    "cursor.execute(index)\n",
    "\n",
    "sq=\"'\"\n",
    "count=0\n",
    "\n",
    "for key,value in new_voc_dict.items():\n",
    "    request='Insert into Vocabulary Values (%s,%d,%d)'%(sq+key+sq,value,somme[0,value])\n",
    "    cursor.execute(request)\n",
    "    count+=1\n",
    "    if count%100000==0:\n",
    "        print(count)\n",
    "conn.commit()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
