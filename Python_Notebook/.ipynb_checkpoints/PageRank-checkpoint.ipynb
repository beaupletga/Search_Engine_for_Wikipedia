{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.random_projection import GaussianRandomProjection\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import os\n",
    "import json\n",
    "import urllib.parse\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix,csc_matrix\n",
    "import operator\n",
    "import pandas as pd\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upper_sentence(sentence):\n",
    "    if len(sentence)>1:\n",
    "        return sentence[0].upper()+sentence[1:]\n",
    "    if len(sentence)==1:\n",
    "        return sentence[0].upper() \n",
    "    return sentence\n",
    "\n",
    "def remove_tag(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "# transform the name of the links into id of article\n",
    "def compute_name_to_id_of_article(title_dict_inverse,link_dic):\n",
    "    for key,value in link_dict.items():\n",
    "        id_list= [title_dict_inverse[i] for i in value if i in title_dict_inverse]\n",
    "        link_dict[key]=id_list\n",
    "    return link_dic\n",
    "\n",
    "# to avoid duplicates in the links\n",
    "# to avoid self link\n",
    "def clean_links(link_dict):\n",
    "    for key,value in link_dict.items():\n",
    "        link_dict[key]=list(set(value))\n",
    "        if key in link_dict[key]:\n",
    "            link_dict[key].remove(key)\n",
    "    return link_dict\n",
    "\n",
    "def construct_inverse_link_dict(link_dict):\n",
    "    link_dict_inverse={}\n",
    "    for key,value in link_dict.items():\n",
    "        for i in value:\n",
    "            if i in link_dict_inverse:\n",
    "                link_dict_inverse[i].append(key)\n",
    "            else:\n",
    "                link_dict_inverse[i]=[key]\n",
    "    return link_dict_inverse\n",
    "\n",
    "# give a line/column number to each article\n",
    "# because we creating the matrix we don't use the id anymore, only the nb of line/row\n",
    "def get_column_id(title_dict):\n",
    "    indices_dict={}\n",
    "    indices_dict_inverse={}\n",
    "    count=0\n",
    "    for i in title_dict:\n",
    "        indices_dict[i]=count\n",
    "        indices_dict_inverse[count]=i\n",
    "        count+=1\n",
    "    return indices_dict,indices_dict_inverse\n",
    "\n",
    "# prepare the 3 list for the sparse matrix\n",
    "def prepare_data_sparse(title_dict,link_dict,indices_dict):\n",
    "    row=[]\n",
    "    column=[]\n",
    "    data=[]\n",
    "    for i in title_dict:\n",
    "        out_links=link_dict[i]\n",
    "        for j in out_links:\n",
    "            row.append(indices_dict[j])\n",
    "            column.append(indices_dict[i])\n",
    "            data.append(1/len(out_links))\n",
    "    return row,column,data\n",
    "\n",
    "# main algorithm\n",
    "# compute the page rank using the matrix M constructed before and returning a vector r of pagerank of each article\n",
    "def page_rank(title_dict,M,BETA=0.85,eps=1e-5):\n",
    "    n=len(title_dict)\n",
    "    r=np.array([1/n for i in range(n)]).reshape(-1,1)\n",
    "    last_r=np.ones((n,1))\n",
    "    while np.linalg.norm(r - last_r, 2) > eps:\n",
    "        last_r=r\n",
    "        r=BETA*M.dot(r)+((1-BETA)/n)*np.ones((n,1))\n",
    "        r/=np.sum(r)\n",
    "    return r\n",
    "\n",
    "def advice_next_article(title=\"None\",id=\"None\"):\n",
    "    if title!=\"None\":\n",
    "        if title in title_dic_inverse:\n",
    "            id=title_dic_inverse[title]\n",
    "            list_input_links=link_dic_inverse[id]\n",
    "        else:\n",
    "            print(\"Title not in the dict\")\n",
    "            return\n",
    "    elif id!=\"None\":\n",
    "        if id in link_dic_inverse:\n",
    "            list_input_links=link_dic_inverse[id]\n",
    "        else:\n",
    "            print(\"Id not in dict\")\n",
    "            return\n",
    "    \n",
    "    list_input_id=[indides_dic[i] for i in list_input_links]\n",
    "    output=[[indides_dic_inverse[i],r[i]] for i in list_input_id]\n",
    "    output=sorted(output, key=lambda x: x[1],reverse=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path='/home/gabriel/Documents/MPRI/Web_Data_Management/wikiextractor-master/text/'\n",
    "title_list=[]\n",
    "text_list=[]\n",
    "title_dict={}\n",
    "title_dict_inverse={}\n",
    "text_dict={}\n",
    "link_dict={}\n",
    "location_dict={}\n",
    "wv_dict={}\n",
    "www_adress={}\n",
    "df=[]\n",
    "count=0\n",
    "for w,i in enumerate(os.listdir(path)):\n",
    "    for j in os.listdir(path+i):\n",
    "        for filename in os.listdir(path+i+'/'+j):\n",
    "            with open(path+i+'/'+j+'/'+filename) as f:\n",
    "                lines = [line.rstrip('\\n') for line in f]\n",
    "            for line_index,line in enumerate(lines):\n",
    "                a=json.loads(line)\n",
    "                if 'text' in a:\n",
    "                    title_dict[a['id']]=a['title']\n",
    "                    title_dict_inverse[a['title']]=a['id']\n",
    "#                     location_dic[a['id']]=path+i+'/'+j+'/'+filename\n",
    "                    www_adress[a['id']]=a['url']\n",
    "#                     text_dict[a['id']]=remove_tag(a['text'][:200])\n",
    "                    urls=re.findall(r'href=[\\'\"]?([^\\'\" >]+)', a['text'])\n",
    "                    title_list_iri=[upper_sentence(urllib.parse.unquote(i)) for i in urls]\n",
    "                    link_dict[a['id']]=title_list_iri\n",
    "                count+=1\n",
    "#                 if count==500000:\n",
    "#                     s=q\n",
    "    \n",
    "#     if w==0:\n",
    "#         break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_dict=compute_name_to_id_of_article(title_dict_inverse,link_dict)\n",
    "link_dict=clean_links(link_dict)\n",
    "link_dict_inverse=construct_inverse_link_dict(link_dict)\n",
    "indices_dict,indices_dict_inverse=get_column_id(title_dict)\n",
    "row,column,data=prepare_data_sparse(title_dict,link_dict,indices_dict)\n",
    "M=csc_matrix((data, (row, column)), shape=(len(title_dict),len(title_dict)))\n",
    "r=page_rank(title_dict,M,BETA=0.85,eps=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "États-Unis 1377936 [0.02383267]\n",
      "Allemagne 1590005 [0.00814473]\n",
      "Québec 1134826 [0.00420563]\n",
      "Union européenne 2143998 [0.00370382]\n",
      "Station de radio 1066079 [0.00211953]\n",
      "Émission de radio 1066006 [0.00205143]\n",
      "Israël 1276998 [0.00185695]\n",
      "Endémisme 1330904 [0.00184704]\n",
      "Agriculture 1967333 [0.00175102]\n",
      "Turquie 1574253 [0.00172003]\n",
      "Colombie 1416234 [0.00161551]\n",
      "Islam 1219912 [0.00156654]\n",
      "Xian (subdivision administrative) 1082298 [0.00147201]\n",
      "Empire byzantin 6873847 [0.00115564]\n",
      "Troisième Reich 1621329 [0.00107608]\n"
     ]
    }
   ],
   "source": [
    "s=sorted(range(len(r)), key=lambda k: r[k],reverse=True)\n",
    "for i in range(15):\n",
    "    print(title_dict[indices_dict_inverse[s[i]]],indices_dict_inverse[s[i]],r[s[i]])\n",
    "# print(title_dic[indides_dic_inverse[np.argmax(r)]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"r.npy\",r)\n",
    "j = json.dumps(indices_dict)\n",
    "f = open(\"indices_dict.json\",\"w\")\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143862\n"
     ]
    }
   ],
   "source": [
    "def save_vectors():\n",
    "    count=0\n",
    "    file = open(\"articles_embedding.tsv\",\"w\")\n",
    "    file2 = open(\"article_title.tsv\",\"w\")\n",
    "    for key,value in wv_dic.items():\n",
    "        if type(value)==np.ndarray:\n",
    "            count+=1\n",
    "            a=[str(i) for i in value]\n",
    "            a='\\t'.join(a)+\"\\n\"\n",
    "            file.write(a)\n",
    "            file2.write(title_dic[key]+\"\\n\")\n",
    "    file.close()\n",
    "    file2.close()\n",
    "    print(count)\n",
    "save_vectors()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
