{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.sparse import coo_matrix,csr_matrix,csc_matrix,save_npz,load_npz\n",
    "import time\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import string\n",
    "from collections import Counter\n",
    "import sqlite3\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib qt\n",
    "from mpl_toolkits.mplot3d import axes3d, Axes3D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This notebook computes the tf-idf matrix C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = str.maketrans(string.punctuation, ' '*len(string.punctuation))\n",
    "stop_words=[\"a\",\"abord\",\"absolument\",\"afin\",\"ah\",\"ai\",\"aie\",\"aient\",\"aies\",\"ailleurs\",\"ainsi\",\"ait\",\"allaient\",\"allo\",\"allons\",\"allô\",\"alors\",\"anterieur\",\"anterieure\",\"anterieures\",\"apres\",\"après\",\"as\",\"assez\",\"attendu\",\"au\",\"aucun\",\"aucune\",\"aucuns\",\"aujourd\",\"aujourd'hui\",\"aupres\",\"auquel\",\"aura\",\"aurai\",\"auraient\",\"aurais\",\"aurait\",\"auras\",\"aurez\",\"auriez\",\"aurions\",\"aurons\",\"auront\",\"aussi\",\"autre\",\"autrefois\",\"autrement\",\"autres\",\"autrui\",\"aux\",\"auxquelles\",\"auxquels\",\"avaient\",\"avais\",\"avait\",\"avant\",\"avec\",\"avez\",\"aviez\",\"avions\",\"avoir\",\"avons\",\"ayant\",\"ayez\",\"ayons\",\"b\",\"bah\",\"bas\",\"basee\",\"bat\",\"beau\",\"beaucoup\",\"bien\",\"bigre\",\"bon\",\"boum\",\"bravo\",\"brrr\",\"c\",\"car\",\"ce\",\"ceci\",\"cela\",\"celle\",\"celle-ci\",\"celle-là\",\"celles\",\"celles-ci\",\"celles-là\",\"celui\",\"celui-ci\",\"celui-là\",\"celà\",\"cent\",\"cependant\",\"certain\",\"certaine\",\"certaines\",\"certains\",\"certes\",\"ces\",\"cet\",\"cette\",\"ceux\",\"ceux-ci\",\"ceux-là\",\"chacun\",\"chacune\",\"chaque\",\"cher\",\"chers\",\"chez\",\"chiche\",\"chut\",\"chère\",\"chères\",\"ci\",\"cinq\",\"cinquantaine\",\"cinquante\",\"cinquantième\",\"cinquième\",\"clac\",\"clic\",\"combien\",\"comme\",\"comment\",\"comparable\",\"comparables\",\"compris\",\"concernant\",\"contre\",\"couic\",\"crac\",\"d\",\"da\",\"dans\",\"de\",\"debout\",\"dedans\",\"dehors\",\"deja\",\"delà\",\"depuis\",\"dernier\",\"derniere\",\"derriere\",\"derrière\",\"des\",\"desormais\",\"desquelles\",\"desquels\",\"dessous\",\"dessus\",\"deux\",\"deuxième\",\"deuxièmement\",\"devant\",\"devers\",\"devra\",\"devrait\",\"different\",\"differentes\",\"differents\",\"différent\",\"différente\",\"différentes\",\"différents\",\"dire\",\"directe\",\"directement\",\"dit\",\"dite\",\"dits\",\"divers\",\"diverse\",\"diverses\",\"dix\",\"dix-huit\",\"dix-neuf\",\"dix-sept\",\"dixième\",\"doit\",\"doivent\",\"donc\",\"dont\",\"dos\",\"douze\",\"douzième\",\"dring\",\"droite\",\"du\",\"duquel\",\"durant\",\"dès\",\"début\",\"désormais\",\"e\",\"effet\",\"egale\",\"egalement\",\"egales\",\"eh\",\"elle\",\"elle-même\",\"elles\",\"elles-mêmes\",\"en\",\"encore\",\"enfin\",\"entre\",\"envers\",\"environ\",\"es\",\"essai\",\"est\",\"et\",\"etant\",\"etc\",\"etre\",\"eu\",\"eue\",\"eues\",\"euh\",\"eurent\",\"eus\",\"eusse\",\"eussent\",\"eusses\",\"eussiez\",\"eussions\",\"eut\",\"eux\",\"eux-mêmes\",\"exactement\",\"excepté\",\"extenso\",\"exterieur\",\"eûmes\",\"eût\",\"eûtes\",\"f\",\"fais\",\"faisaient\",\"faisant\",\"fait\",\"faites\",\"façon\",\"feront\",\"fi\",\"flac\",\"floc\",\"fois\",\"font\",\"force\",\"furent\",\"fus\",\"fusse\",\"fussent\",\"fusses\",\"fussiez\",\"fussions\",\"fut\",\"fûmes\",\"fût\",\"fûtes\",\"g\",\"gens\",\"h\",\"ha\",\"haut\",\"hein\",\"hem\",\"hep\",\"hi\",\"ho\",\"holà\",\"hop\",\"hormis\",\"hors\",\"hou\",\"houp\",\"hue\",\"hui\",\"huit\",\"huitième\",\"hum\",\"hurrah\",\"hé\",\"hélas\",\"i\",\"ici\",\"il\",\"ils\",\"importe\",\"j\",\"je\",\"jusqu\",\"jusque\",\"juste\",\"k\",\"l\",\"la\",\"laisser\",\"laquelle\",\"las\",\"le\",\"lequel\",\"les\",\"lesquelles\",\"lesquels\",\"leur\",\"leurs\",\"longtemps\",\"lors\",\"lorsque\",\"lui\",\"lui-meme\",\"lui-même\",\"là\",\"lès\",\"m\",\"ma\",\"maint\",\"maintenant\",\"mais\",\"malgre\",\"malgré\",\"maximale\",\"me\",\"meme\",\"memes\",\"merci\",\"mes\",\"mien\",\"mienne\",\"miennes\",\"miens\",\"mille\",\"mince\",\"mine\",\"minimale\",\"moi\",\"moi-meme\",\"moi-même\",\"moindres\",\"moins\",\"mon\",\"mot\",\"moyennant\",\"multiple\",\"multiples\",\"même\",\"mêmes\",\"n\",\"na\",\"naturel\",\"naturelle\",\"naturelles\",\"ne\",\"neanmoins\",\"necessaire\",\"necessairement\",\"neuf\",\"neuvième\",\"ni\",\"nombreuses\",\"nombreux\",\"nommés\",\"non\",\"nos\",\"notamment\",\"notre\",\"nous\",\"nous-mêmes\",\"nouveau\",\"nouveaux\",\"nul\",\"néanmoins\",\"nôtre\",\"nôtres\",\"o\",\"oh\",\"ohé\",\"ollé\",\"olé\",\"on\",\"ont\",\"onze\",\"onzième\",\"ore\",\"ou\",\"ouf\",\"ouias\",\"oust\",\"ouste\",\"outre\",\"ouvert\",\"ouverte\",\"ouverts\",\"o|\",\"où\",\"p\",\"paf\",\"pan\",\"par\",\"parce\",\"parfois\",\"parle\",\"parlent\",\"parler\",\"parmi\",\"parole\",\"parseme\",\"partant\",\"particulier\",\"particulière\",\"particulièrement\",\"pas\",\"passé\",\"pendant\",\"pense\",\"permet\",\"personne\",\"personnes\",\"peu\",\"peut\",\"peuvent\",\"peux\",\"pff\",\"pfft\",\"pfut\",\"pif\",\"pire\",\"pièce\",\"plein\",\"plouf\",\"plupart\",\"plus\",\"plusieurs\",\"plutôt\",\"possessif\",\"possessifs\",\"possible\",\"possibles\",\"pouah\",\"pour\",\"pourquoi\",\"pourrais\",\"pourrait\",\"pouvait\",\"prealable\",\"precisement\",\"premier\",\"première\",\"premièrement\",\"pres\",\"probable\",\"probante\",\"procedant\",\"proche\",\"près\",\"psitt\",\"pu\",\"puis\",\"puisque\",\"pur\",\"pure\",\"q\",\"qu\",\"quand\",\"quant\",\"quant-à-soi\",\"quanta\",\"quarante\",\"quatorze\",\"quatre\",\"quatre-vingt\",\"quatrième\",\"quatrièmement\",\"que\",\"quel\",\"quelconque\",\"quelle\",\"quelles\",\"quelqu'un\",\"quelque\",\"quelques\",\"quels\",\"qui\",\"quiconque\",\"quinze\",\"quoi\",\"quoique\",\"r\",\"rare\",\"rarement\",\"rares\",\"relative\",\"relativement\",\"remarquable\",\"rend\",\"rendre\",\"restant\",\"reste\",\"restent\",\"restrictif\",\"retour\",\"revoici\",\"revoilà\",\"rien\",\"s\",\"sa\",\"sacrebleu\",\"sait\",\"sans\",\"sapristi\",\"sauf\",\"se\",\"sein\",\"seize\",\"selon\",\"semblable\",\"semblaient\",\"semble\",\"semblent\",\"sent\",\"sept\",\"septième\",\"sera\",\"serai\",\"seraient\",\"serais\",\"serait\",\"seras\",\"serez\",\"seriez\",\"serions\",\"serons\",\"seront\",\"ses\",\"seul\",\"seule\",\"seulement\",\"si\",\"sien\",\"sienne\",\"siennes\",\"siens\",\"sinon\",\"six\",\"sixième\",\"soi\",\"soi-même\",\"soient\",\"sois\",\"soit\",\"soixante\",\"sommes\",\"son\",\"sont\",\"sous\",\"souvent\",\"soyez\",\"soyons\",\"specifique\",\"specifiques\",\"speculatif\",\"stop\",\"strictement\",\"subtiles\",\"suffisant\",\"suffisante\",\"suffit\",\"suis\",\"suit\",\"suivant\",\"suivante\",\"suivantes\",\"suivants\",\"suivre\",\"sujet\",\"superpose\",\"sur\",\"surtout\",\"t\",\"ta\",\"tac\",\"tandis\",\"tant\",\"tardive\",\"te\",\"tel\",\"telle\",\"tellement\",\"telles\",\"tels\",\"tenant\",\"tend\",\"tenir\",\"tente\",\"tes\",\"tic\",\"tien\",\"tienne\",\"tiennes\",\"tiens\",\"toc\",\"toi\",\"toi-même\",\"ton\",\"touchant\",\"toujours\",\"tous\",\"tout\",\"toute\",\"toutefois\",\"toutes\",\"treize\",\"trente\",\"tres\",\"trois\",\"troisième\",\"troisièmement\",\"trop\",\"très\",\"tsoin\",\"tsouin\",\"tu\",\"té\",\"u\",\"un\",\"une\",\"unes\",\"uniformement\",\"unique\",\"uniques\",\"uns\",\"v\",\"va\",\"vais\",\"valeur\",\"vas\",\"vers\",\"via\",\"vif\",\"vifs\",\"vingt\",\"vivat\",\"vive\",\"vives\",\"vlan\",\"voici\",\"voie\",\"voient\",\"voilà\",\"vont\",\"vos\",\"votre\",\"vous\",\"vous-mêmes\",\"vu\",\"vé\",\"vôtre\",\"vôtres\",\"w\",\"x\",\"y\",\"z\",\"zut\",\"à\",\"â\",\"ça\",\"ès\",\"étaient\",\"étais\",\"était\",\"étant\",\"état\",\"étiez\",\"étions\",\"été\",\"étée\",\"étées\",\"étés\",\"êtes\",\"être\",\"ô\"]\n",
    "stop_words+=[\"»\",\"«\",\"''\",\" \",\"–\"]\n",
    "stop_words=set(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_tag(text):\n",
    "    cleanr = re.compile('<.*?>')\n",
    "    cleantext = re.sub(cleanr, '', text)\n",
    "    return cleantext\n",
    "\n",
    "def get_counters(text):\n",
    "    text_without_tag=remove_tag(text)\n",
    "#     text_without_tag=''.join([i for i in text_without_tag if not i.isdigit()])\n",
    "    text_split=text_without_tag.translate(table).lower().replace('\\n', ' ').split(' ')\n",
    "    text_without_tag=[i for i in text_split if i not in stop_words and i!=\"\"]\n",
    "#     bigrams=[text_without_tag[i]+\" \"+text_without_tag[i+1] for i in range(0,len(text_without_tag)-1)\n",
    "#             if text_without_tag[i]!='' and text_without_tag[i+1]!='']\n",
    "    \n",
    "    counter_text=Counter(text_without_tag)\n",
    "#     counter_bigrams=Counter(bigrams)\n",
    "    counter_bigrams={}\n",
    "    return counter_text,counter_bigrams\n",
    "\n",
    "\n",
    "def clean_dataset(voc_dict,voc_dict_inverse,voc_dict_triplet):\n",
    "    index_to_remove=set([key for key in voc_dict_triplet.keys() if len(voc_dict_triplet[key])==1])\n",
    "    voc_dict={key:value for key,value in voc_dict.items() if value not in index_to_remove}\n",
    "    voc_dict_inverse={key:value for key,value in voc_dict_inverse.items() if key not in index_to_remove}\n",
    "    voc_dict_triplet={key:value for key,value in voc_dict_triplet.items() if key not in index_to_remove}\n",
    "    return voc_dict,voc_dict_inverse,voc_dict_triplet\n",
    "\n",
    "\n",
    "def get_matrix(voc_dict_triplet):\n",
    "    row=[]\n",
    "    column=[]\n",
    "    data=[]\n",
    "    index=0\n",
    "    new_voc_dict={}\n",
    "    for key,value in voc_dict_triplet.items():\n",
    "        new_voc_dict[key]=index\n",
    "        for duo in value:\n",
    "            for id_ in duo[0]:\n",
    "                row.append(id_)\n",
    "                column.append(index)\n",
    "                data.append(duo[1])\n",
    "        index+=1\n",
    "    return csc_matrix((data, (row, column)), shape=(np.max(row)+1,len(voc_dict_triplet))),new_voc_dict\n",
    "\n",
    "def increase_title_weights(B,M,dataset,new_voc_dict):\n",
    "    B=B.tolil()\n",
    "    for i in range(M.shape[0]):\n",
    "        article_title=dataset[i][1]\n",
    "        counter_text,counter_bigrams=get_counters(article_title)\n",
    "        index=[new_voc_dict[key] for counter in [counter_text,counter_bigrams] for key in counter.keys() if key in new_voc_dict]\n",
    "        for j in index:\n",
    "            B[i,j]=10*B[i,j]\n",
    "    B=B.tocsc()\n",
    "    return B\n",
    "    \n",
    "# def increase_title_weights2(C):\n",
    "#     for i in range(C.shape[0]):\n",
    "#         article_title=dataset[i][1]\n",
    "#         counter_text,counter_bigrams=get_counters(article_title)\n",
    "#         if len(counter_text)==1:\n",
    "#             for key in counter_text.keys():\n",
    "#                 if key in new_voc_dict:\n",
    "#                     C[i,new_voc_dict[key]]=2*C[i,new_voc_dict[key]]\n",
    "#         if i%10000==0:\n",
    "#             print(i)\n",
    "#     return C    \n",
    "    \n",
    "def get_C(M,dataset,new_voc_dict):\n",
    "    B=M.copy()\n",
    "    B.data=np.log10(B.data)+1\n",
    "    B=increase_title_weights(B,M,dataset,new_voc_dict)\n",
    "    A=count/np.sum(M>0,axis=0)\n",
    "    A.data=np.log10(A.data)\n",
    "    C=B.multiply(A).astype(\"float16\")\n",
    "    del A\n",
    "    del B\n",
    "    return C\n",
    "\n",
    "def get_closest_articles(C,M,new_voc_dict,dataset,query,k=50):\n",
    "    counter_words,counter_bigrams=get_counters(query)\n",
    "    keys=[]\n",
    "    tf_idf=[]\n",
    "    for counter in [counter_words,counter_bigrams]:\n",
    "        for key,value in counter.items():\n",
    "            if key in new_voc_dict:\n",
    "                tf=1+np.log10(value/(len(counter_words)+len(counter_bigrams)))\n",
    "                idf=np.log10(C.shape[0]/np.sum(M.getcol(new_voc_dict[key])))\n",
    "                tf_idf.append(tf*idf)\n",
    "                keys.append(new_voc_dict[key])\n",
    "    dist = np.array((C[:,keys] + np.array(tf_idf).reshape(1,len(tf_idf))))\n",
    "    dist = np.sum(dist**2, axis=1)\n",
    "    argsort=np.argsort(dist)[::-1]\n",
    "    return [dataset[argsort[i]] for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "100000 117671 6144.09375\n",
      "6.0\n",
      "200000 129714 6144.09375\n",
      "7.0\n",
      "300000 146956 6144.09375\n",
      "8.0\n",
      "400000 160189 6144.09375\n",
      "10.0\n",
      "500000 161741 6144.09375\n",
      "14.0\n",
      "600000 151440 6144.09375\n",
      "6.0\n",
      "700000 184583 12288.09375\n",
      "8.0\n",
      "800000 199843 12288.09375\n",
      "25.0\n",
      "900000 147096 6144.09375\n",
      "32.0\n",
      "1000000 136979 6144.09375\n",
      "26.0\n",
      "1100000 137777 6144.09375\n",
      "16.0\n",
      "1200000 140099 6144.09375\n",
      "11.0\n",
      "1300000 144036 6144.09375\n",
      "8.0\n",
      "1400000 155217 6144.09375\n",
      "12.0\n",
      "1500000 155238 6144.09375\n",
      "42.0\n",
      "1600000 136667 6144.09375\n",
      "5.0\n",
      "1700000 172004 6144.09375\n",
      "5.0\n",
      "1800000 229944 12288.09375\n",
      "26.0\n",
      "1900000 148061 6144.09375\n",
      "37.0\n",
      "2000000 142784 6144.09375\n"
     ]
    }
   ],
   "source": [
    "path='/home/gabriel/Documents/MPRI/Web_Data_Management/wikiextractor-master/text/'\n",
    "count=0\n",
    "dataset={}\n",
    "voc_dict_triplet={}\n",
    "index=0\n",
    "\n",
    "for w,i in enumerate(os.listdir(path)):\n",
    "    for j in os.listdir(path+i):\n",
    "        for filename in os.listdir(path+i+'/'+j):\n",
    "            with open(path+i+'/'+j+'/'+filename) as f:\n",
    "                lines = [line.rstrip('\\n') for line in f]\n",
    "            for line_index,line in enumerate(lines):\n",
    "                a=json.loads(line) \n",
    "                if 'text' in a:\n",
    "                    counter_text,counter_bigrams=get_counters(a['text'])\n",
    "                    for counter in [counter_text]:\n",
    "                        for key,value in counter.items():\n",
    "                            if key not in voc_dict_triplet:\n",
    "                                voc_dict_triplet[key]=[([count],value)]\n",
    "                                index+=1\n",
    "                            else:\n",
    "                                add=False\n",
    "                                for qq in voc_dict_triplet[key]:\n",
    "                                    if qq[1]==value:\n",
    "                                        qq[0].append(count)\n",
    "                                        add=True\n",
    "                                        break\n",
    "                                if add==False:\n",
    "                                    voc_dict_triplet[key].append(([count],value))\n",
    "                                    \n",
    "                    location=path+i+'/'+j+'/'+filename\n",
    "                    dataset[count]=[a['id'],a['title'],a['url'],location,line_index]\n",
    "                    count+=1\n",
    "                        \n",
    "                    if count%100000==0:\n",
    "                        somme=lambda  x:sum([i[1]*len(i[0]) for i in x])\n",
    "                        \n",
    "                        quartile1=np.percentile([somme(value) for key,value in voc_dict_triplet.items()],70)                        \n",
    "                        index_to_remove=set([key for key,value in voc_dict_triplet.items() if (somme(value)<=quartile1)])\n",
    "                        print(quartile1)\n",
    "                        voc_dict_triplet={key:value for key,value in voc_dict_triplet.items() if key not in index_to_remove}\n",
    "                        \n",
    "                        print(count,len(voc_dict_triplet),sys.getsizeof(voc_dict_triplet)/1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second Step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gabriel/.local/lib/python3.5/site-packages/ipykernel_launcher.py:84: DeprecationWarning: Assigning the 'data' attribute is an inherently unsafe operation and will be removed in the future.\n"
     ]
    }
   ],
   "source": [
    "M,new_voc_dict=get_matrix(voc_dict_triplet)\n",
    "del voc_dict_triplet\n",
    "print(\"Second Step\")\n",
    "C=get_C(M,dataset,new_voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz('../Database/C.npz', C)\n",
    "save_npz('../Database/M.npz', M)\n",
    "j = json.dumps(dataset)\n",
    "f = open(\"../Database/dataset.json\",\"w\")\n",
    "f.write(j)\n",
    "f.close()\n",
    "j = json.dumps(new_voc_dict)\n",
    "f = open(\"../Database/new_voc_dict.json\",\"w\")\n",
    "f.write(j)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = load_npz(\"../Database/C.npz\")\n",
    "# M = load_npz(\"../Database/M.npz\")\n",
    "f = open('../Database/new_voc_dict.json')\n",
    "new_voc_dict = json.load(f)\n",
    "f.close()\n",
    "f = open('../Database/dataset.json')\n",
    "dataset = json.load(f)\n",
    "f.close()\n",
    "dataset={int(key):[int(value[0]),value[1],value[2],value[3],value[4]] for key,value in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "query=\"théorie des graphes et des cliques et des graphes complets et des graphes bipartis\"\n",
    "result_list=get_closest_articles(C,M,new_voc_dict,dataset,query,k=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
